<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://liangsj.xyz/</id>
    <title>Liangsj</title>
    <updated>2019-07-05T09:23:05.101Z</updated>
    <generator>https://github.com/jpmonette/feed</generator>
    <link rel="alternate" href="https://liangsj.xyz/"/>
    <link rel="self" href="https://liangsj.xyz//atom.xml"/>
    <subtitle>个人博客</subtitle>
    <logo>https://liangsj.xyz//images/avatar.png</logo>
    <icon>https://liangsj.xyz//favicon.ico</icon>
    <rights>All rights reserved 2019, Liangsj</rights>
    <entry>
        <title type="html"><![CDATA[Spark Helloworld]]></title>
        <id>https://liangsj.xyz//post/spark-helloworld</id>
        <link href="https://liangsj.xyz//post/spark-helloworld">
        </link>
        <updated>2019-07-05T08:24:31.000Z</updated>
        <content type="html"><![CDATA[<p>MapReduce 适合数据量非常大，对延迟要求不高，计算相当简单的情境。</p>
<p>spark 是基于内存计算的大数据并行计算框架，速度快，适合于数据较大，业务较为复杂，而又不能延迟太久的情境。</p>
<p>MR1主要由JobTracker和TaskTacker组成，Yarn由ResourceManager、NodeManager、ApplicationMaster组成。</p>
<p>下面开始Spark的学习。</p>
<p>这里已经使用Cloudera Manager搭建好Hadoop和Spark环境。</p>
<p>以下为Spark的HelloWorld，输入为2个文件，文件有几个单词，通过Spark计算出单词的个数</p>
<h3 id="创建测试文件">创建测试文件</h3>
<ul>
<li>创建文件 file01和file01，内容如下
file01</li>
</ul>
<pre><code>Hello World Bye World
Hello2 World2 Bye2 World2
</code></pre>
<p>file02</p>
<pre><code>Hello Hadoop Goodbye Hadoop
</code></pre>
<ul>
<li>上传到HDFS</li>
</ul>
<pre><code>$ hadoop fs -mkdir -p /tmp/wordcount/input/
$ hadoop fs -put file0* /tmp/wordcount/input/
</code></pre>
<h3 id="使用spark-shell的helloworld">使用spark shell的HelloWorld</h3>
<pre><code>$ spark-shell
&gt; var lines = sc.textFile(&quot;/tmp/wordcount/input/&quot;)
&gt; lines.collect().foreach(println)
Hello World Bye World
Hello2 World2 Bye2 World2
Hello Hadoop Goodbye Hadoop
&gt; var result=lines.flatMap(line=&gt;line.split(&quot; &quot;)).map(word=&gt;(word,1)).reduceByKey(_+_)
&gt; result.collect().foreach(println)
(Bye,1)
(Hello2,1)
(World,2)
(World2,2)
(Goodbye,1)
(Hello,2)
(Bye2,1)
(Hadoop,2)
</code></pre>
<h3 id="使用eclipse的helloworld">使用eclipse的HelloWorld</h3>
<ul>
<li>到<a href="http://scala-ide.org/">scala IDE官网</a>下载scala IDE，并安装
使用java开发的话，直接用eclipse就可以了，不需有scala IDE，这里使用scala开发</li>
<li>安装好并配置好maven</li>
<li>新建maven项目
Spark 1.6 对应 Scala 2.10
Spark 2.0 对应 Scala 2.11
这里使用spark是1.6，所以Scala library如果不是2.10，要改为2.10</li>
</ul>
<p>可通过 右键项目-&gt; build Path -&gt; configure build Path -&gt; 删除当前scala library -&gt; Add Library -&gt; Scala Library -&gt; 选择2.10版本</p>
<p><img src="https://liangsj.xyz//post-images/1562315114563.png" alt=""></p>
<p>pom.xml内容如下</p>
<pre><code>&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;
  xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;
  &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;

 &lt;groupId&gt;org.ace&lt;/groupId&gt;
 &lt;artifactId&gt;sparktest&lt;/artifactId&gt;
  &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;
  &lt;name&gt;${project.artifactId}&lt;/name&gt;
  &lt;description&gt;My wonderfull scala app&lt;/description&gt;
  &lt;inceptionYear&gt;2010&lt;/inceptionYear&gt;
  &lt;licenses&gt;
    &lt;license&gt;
      &lt;name&gt;My License&lt;/name&gt;
      &lt;url&gt;http://....&lt;/url&gt;
      &lt;distribution&gt;repo&lt;/distribution&gt;
    &lt;/license&gt;
  &lt;/licenses&gt;

  &lt;properties&gt;
    &lt;maven.compiler.source&gt;1.5&lt;/maven.compiler.source&gt;
    &lt;maven.compiler.target&gt;1.5&lt;/maven.compiler.target&gt;
    &lt;encoding&gt;UTF-8&lt;/encoding&gt;
  &lt;/properties&gt;
  
  &lt;dependencies&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;org.scala-lang&lt;/groupId&gt;
      &lt;artifactId&gt;scala-library&lt;/artifactId&gt;
      &lt;version&gt;2.10.6&lt;/version&gt;
    &lt;/dependency&gt;
    
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;
      &lt;artifactId&gt;spark-core_2.10&lt;/artifactId&gt;
      &lt;version&gt;1.6.2&lt;/version&gt;
    &lt;/dependency&gt;
    &lt;!-- Test --&gt;
    &lt;dependency&gt;
      &lt;groupId&gt;junit&lt;/groupId&gt;
      &lt;artifactId&gt;junit&lt;/artifactId&gt;
      &lt;version&gt;4.8.1&lt;/version&gt;
      &lt;scope&gt;test&lt;/scope&gt;
    &lt;/dependency&gt;
  &lt;/dependencies&gt;

  &lt;build&gt;
    &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt;
    &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt;
    &lt;plugins&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.scala-tools&lt;/groupId&gt;
        &lt;artifactId&gt;maven-scala-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.15.0&lt;/version&gt;
        &lt;executions&gt;
          &lt;execution&gt;
            &lt;goals&gt;
              &lt;goal&gt;compile&lt;/goal&gt;
              &lt;goal&gt;testCompile&lt;/goal&gt;
            &lt;/goals&gt;
            &lt;configuration&gt;
              &lt;args&gt;
                &lt;arg&gt;-make:transitive&lt;/arg&gt;
                &lt;arg&gt;-dependencyfile&lt;/arg&gt;
                &lt;arg&gt;${project.build.directory}/.scala_dependencies&lt;/arg&gt;
              &lt;/args&gt;
            &lt;/configuration&gt;
          &lt;/execution&gt;
        &lt;/executions&gt;
      &lt;/plugin&gt;
      &lt;plugin&gt;
        &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;
        &lt;artifactId&gt;maven-surefire-plugin&lt;/artifactId&gt;
        &lt;version&gt;2.6&lt;/version&gt;
        &lt;configuration&gt;
          &lt;useFile&gt;false&lt;/useFile&gt;
          &lt;disableXmlReport&gt;true&lt;/disableXmlReport&gt;
          &lt;!-- If you have classpath issue like NoDefClassError,... --&gt;
          &lt;!-- useManifestOnlyJar&gt;false&lt;/useManifestOnlyJar --&gt;
          &lt;includes&gt;
            &lt;include&gt;**/*Test.*&lt;/include&gt;
            &lt;include&gt;**/*Suite.*&lt;/include&gt;
          &lt;/includes&gt;
        &lt;/configuration&gt;
      &lt;/plugin&gt;
    &lt;/plugins&gt;
  &lt;/build&gt;
&lt;/project&gt;
</code></pre>
<p>右键项目 -&gt; maven -&gt; update project</p>
<p>等待项目构建完成</p>
<ul>
<li>新建Scala Object
内容为</li>
</ul>
<pre><code>package org.ace.spark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object WordCount {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(&quot;workCount&quot;);
    val sc = new SparkContext(conf);
    val textFile = sc.textFile(&quot;/tmp/wordcount/input/&quot;)
    val counts = textFile.flatMap(line =&gt; line.split(&quot; &quot;))
                 .map(word =&gt; (word, 1))
                 .reduceByKey(_ + _)
    counts.saveAsTextFile(&quot;/tmp/wordcount/output&quot;)
  }
}
</code></pre>
<p>(这里为打包到Linux运行，直接在eclipse运行也可以，请看这里 <a href="http://liangsj.xyz/2017/09/sparkOnEclipse/">eclipse运行Spark程序</a>)</p>
<ul>
<li>打包
右键src文件夹 -&gt; export -&gt; JAR file -&gt; 导出jar包</li>
<li>上传到spark环境的linux</li>
<li>运行</li>
</ul>
<pre><code>spark-submit \
  --class org.ace.spark.WordCount \
  --master local \
  --num-executors 2 \
  --executor-memory 1g \
  --executor-cores 4  \
  sparkhelloworld.jar 
</code></pre>
<ul>
<li>查看结果</li>
</ul>
<pre><code>$ hadoop fs -ls /tmp/wordcount/output/
$ hadoop fs -cat /tmp/wordcount/output/part*
</code></pre>
<p><img src="https://liangsj.xyz//post-images/1562315125460.png" alt=""></p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[spark Action]]></title>
        <id>https://liangsj.xyz//post/spark-action</id>
        <link href="https://liangsj.xyz//post/spark-action">
        </link>
        <updated>2019-07-05T08:24:02.000Z</updated>
        <content type="html"><![CDATA[<p>Action 返回计算结果</p>
<h3 id="reducef">reduce(f)</h3>
<p>接收一个函数，作用在RDD两个类型相同的元素上，返回新元素
可以实现，RDD中元素的累加，计数，和其它类型的聚集操作</p>
<pre><code>&gt;val rdd=sc.parallelize(Array(1,2,3,4))
&gt;val sum=rdd.reduce((x,y) =&gt; x+y)
sum: Int = 10
</code></pre>
<h3 id="foldzerovalueop">fold(zeroValue)(op)</h3>
<p>和reduce原理相同，但与reduce不同，相当于每个reducejf ,迭代器取的第一个元素是zeroValue</p>
<pre><code>&gt; val sum=rdd.fold(0)((x,y) =&gt; x+y )
sum: Int = 10
0+rdd(1)+rdd(2)+rdd(3)+rdd(4)
</code></pre>
<h3 id="collect">collect()</h3>
<p>遍历整个RDD，返回为一个单机的scala Array数组。一般测试时使用</p>
<pre><code>&gt;var lines=sc.parallelize(Array(&quot;a&quot;,&quot;b&quot;,&quot;c&quot;))
&gt;lines.collect.foreach(println)
a
b
c
</code></pre>
<h3 id="taken">take(n)</h3>
<p>返回RDD的n个元素（同时尝试访问最少的paritions）
返回结果无序，测试使用</p>
<h3 id="topn">top(n)</h3>
<p>取排序后前几位，使用默认的比较器</p>
<pre><code>&gt;var nums=sc.parallelize(Array(1,3,6,4))
&gt;nums.top(2)
res47: Array[Int] = Array(6, 4)
</code></pre>
<h3 id="foreachf">foreach(f)</h3>
<p>计算RDD中的每个元素，但不返回到本地
可以配合println()友好的打印出数据</p>
<pre><code>&gt;lines.collect.foreach(println)
</code></pre>
<h3 id="saveastextfilepathstring">saveAsTextFile(path:String)</h3>
<p>把数据输出，存储到HDFS的指定目录</p>
<pre><code>&gt;lines.saveAsTextFile(&quot;/tmp/sparkout&quot;)
</code></pre>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[eclipse运行Spark程序]]></title>
        <id>https://liangsj.xyz//post/eclipse-yun-xing-spark-cheng-xu</id>
        <link href="https://liangsj.xyz//post/eclipse-yun-xing-spark-cheng-xu">
        </link>
        <updated>2019-07-05T08:22:18.000Z</updated>
        <content type="html"><![CDATA[<p>Spark运行比MapReduce简单，搭建好Scala环境就基本可以了</p>
<h3 id="一-搭建spark开发环境">一、搭建Spark开发环境</h3>
<p>见<a href="http://liangsj.xyz/2017/09/spark-helloworld/">《spark helloworld》</a>里的 &quot;使用eclipse的HelloWorld&quot;</p>
<h3 id="二-编写spark程序">二、编写Spark程序</h3>
<pre><code>package org.ace.spark

import org.apache.spark.SparkConf
import org.apache.spark.SparkContext

object WordCount {
  def main(args: Array[String]): Unit = {
    val conf = new SparkConf().setAppName(&quot;workCount&quot;);
    val sc = new SparkContext(conf);
    val textFile = sc.textFile(args(0))
    val counts = textFile.flatMap(line =&gt; line.split(&quot; &quot;))
                 .map(word =&gt; (word, 1))
                 .reduceByKey(_ + _)
    counts.saveAsTextFile(args(1))
  }
}
</code></pre>
<h3 id="三-运行spark">三、运行Spark</h3>
<p>右键 -&gt; run as -&gt; Run Configurations -&gt; Arguments</p>
<p>指定输入输出时，带上hdfs的IP和端口</p>
<p>如使用我本机的hdfs(也可以远程机器的hdfs)</p>
<pre><code>hdfs://localhost:8020/project/wordcount/input
hdfs://localhost:8020/project/wordcount/output
</code></pre>
<p>并配置VM参数,为master=local,即本地运行模式</p>
<pre><code>-Dspark.master=local
</code></pre>
<p><img src="https://liangsj.xyz//post-images/1562314971328.png" alt=""></p>
<p>点击 run 运行
<img src="https://liangsj.xyz//post-images/1562314977994.png" alt=""></p>
<p>查看HDFS输出</p>
<p><img src="https://liangsj.xyz//post-images/1562314985505.png" alt=""></p>
<p>结束</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[windows搭建hadoop环境及在eclipse运行MapReduce]]></title>
        <id>https://liangsj.xyz//post/windows-da-jian-hadoop-huan-jing-ji-zai-eclipse-yun-xing-mapreduce</id>
        <link href="https://liangsj.xyz//post/windows-da-jian-hadoop-huan-jing-ji-zai-eclipse-yun-xing-mapreduce">
        </link>
        <updated>2019-07-05T08:19:30.000Z</updated>
        <content type="html"><![CDATA[<p>我们开发环境一般为windows,而hadoop是运行在linux，开发时调试如果每次打包好到linux运行，很费时，也很麻烦，效率低</p>
<p>这里教大家如何在windows搭建hadoop环境及在eclipse运行MapReduce</p>
<h3 id="一-下载相关包">一、下载相关包</h3>
<p>1、hadoop（最好下载2.5、2.6或2.7版本的，因为第2点的windows执行文件是对应hadoop2.7版本，亲测2.5和2.65也可以，不确定基本版本是否可用）</p>
<p><a href="http://www.apache.org/dyn/closer.cgi/hadoop/common/hadoop-2.6.5/hadoop-2.6.5.tar.gz">官网下载地址</a></p>
<p>2、windows执行文件（里边包含winutils.exe和一些dll文件）<br>
hadooponwindows-master.zip<br>
已分享到<a href="https://pan.baidu.com/s/1kVAAoE7">百度网盘</a></p>
<p>3、eclipse插件 hadoop-eclipse-plugin-2.6.0.jar</p>
<p>已分享到<a href="https://pan.baidu.com/s/1boOh6Qb">百度网盘</a></p>
<h3 id="二-安装hadoop">二、安装hadoop</h3>
<p>1、安装JDK，这里步骤省略，不会有可以百度，很简单</p>
<p>2、解压hadoop-2.6.5.tar.gz包，解压hadooponwindows-master.zip （路径不要有中文和空格）</p>
<p>3、复制hadooponwindows-master/bin目录里的exe文件和dll文件(一共三个)到hadoop-2.6.5/bin目录</p>
<p>4、配置JAVA_HOME和HADOOP_HOME环境变量<br>
右键我的电脑 -&gt; 高级系统设置 -&gt; 环境变量 -&gt; 系统变量 -&gt; 新建，增加JAVA_HOME和HADOOP_HOME</p>
<p>并把%HADOOP_HOME%\bin和%HADOOP_HOME%\sbin加入到 PATH变量中</p>
<p><img src="https://liangsj.xyz//post-images/1562314808857.png" alt=""><br>
<img src="https://liangsj.xyz//post-images/1562314814916.png" alt=""></p>
<p>5、 配置Hadoop<br>
文件在Hadoop目录的etc/hadoop下
core-site.xml</p>
<p>（路径不要有中文和空格）</p>
<pre><code>&lt;configuration&gt;
   &lt;property&gt;
       &lt;name&gt;fs.defaultFS&lt;/name&gt;
       &lt;value&gt;hdfs://0.0.0.0:19000&lt;/value&gt;
   &lt;/property&gt;
   &lt;property&gt;
        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
        &lt;value&gt;/D:/soft/dev/hadoop/data/tmp&lt;/value&gt;
    &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>hdfs-site.xml</p>
<pre><code>  &lt;configuration&gt;
       &lt;property&gt;
               &lt;name&gt;dfs.replication&lt;/name&gt;
                &lt;value&gt;1&lt;/value&gt;
       &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>mapred-site.xml</p>
<p>(没有的话就复制mapred-site.xml.template为mapred-site.xml)</p>
<pre><code>&lt;configuration&gt;
       &lt;property&gt;
          &lt;name&gt;mapreduce.framework.name&lt;/name&gt;
          &lt;value&gt;yarn&lt;/value&gt;
       &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>yarn-site.xml</p>
<pre><code>&lt;configuration&gt;
       &lt;property&gt;
          &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;
          &lt;value&gt;mapreduce_shuffle&lt;/value&gt;
       &lt;/property&gt;

       &lt;property&gt;
          &lt;name&gt;yarn.nodemanager.aux-services.mapreduce.shuffle.class&lt;/name&gt;
          &lt;value&gt;org.apache.hadoop.mapred.ShuffleHandler&lt;/value&gt;
       &lt;/property&gt;
&lt;/configuration&gt;
</code></pre>
<p>hadoop-env.cmd</p>
<p>修改文件中JAVA_HOME为实际JAVA路径（不能有空格，如果java安装在Program Files下，复制到其它没有空格的目录，再使用该目录）</p>
<pre><code>@rem set JAVA_HOME=%JAVA_HOME%
set JAVA_HOME=D:/Java/jdk1.8.0_101
</code></pre>
<h3 id="三-运行hadoop">三、运行Hadoop</h3>
<p>1、运行cmd窗口，执行以下命令</p>
<pre><code>hdfs namenode -format
</code></pre>
<p>2、启动hdfs</p>
<pre><code> start-dfs
</code></pre>
<p>会启动2个窗口，没异常即为安装成功
<img src="https://liangsj.xyz//post-images/1562314830299.png" alt=""></p>
<p>3、可以查看 HDFS数据</p>
<pre><code>hadoop fs -ls /
</code></pre>
<p>4、访问 http://localhost:50070 可以查看hdfs状态
<img src="https://liangsj.xyz//post-images/1562314842170.png" alt=""></p>
<h3 id="四-配置eclipse">四、配置eclipse</h3>
<p>1、复制hadoop-eclipse-plugin-2.6.0.jar到eclipse dropins目录</p>
<p>2、启动eclipse
打开 window -&gt; Preferences，可以看到多了Hadoop Map/Reduce配置项
配置Hadoop installation directory，如下
<img src="https://liangsj.xyz//post-images/1562314850573.png" alt=""></p>
<p>3、配置hdfs</p>
<p>1）打开 Window -&gt; perspective -&gt; open perspective -&gt; other..
<img src="https://liangsj.xyz//post-images/1562314859406.png" alt=""></p>
<p>2）project explorer出现 DFS Locations
<img src="https://liangsj.xyz//post-images/1562314869656.png" alt=""></p>
<p>3） 打开右下方的Map/Reduce Locations
<img src="https://liangsj.xyz//post-images/1562314881521.png" alt=""></p>
<p>4） 右键 空白处 -&gt; new hadoop location
配置如下，9001为默认 Master端口，应该上面没配置，所以写9001
DFS Master端口为core-site.xml 中配置HDFS端口 19000
<img src="https://liangsj.xyz//post-images/1562314890896.png" alt=""></p>
<p>5）配置成功的话，左右窗口就可以看HDFS目录，可以对HDFS进行操作，我新建了几个目录和上传了文件</p>
<p><img src="https://liangsj.xyz//post-images/1562314898795.png" alt=""></p>
<h3 id="五-运行mapreduce程序">五、运行MapReduce程序</h3>
<p>指定输入和输出，要带上hdfs
hdfs://localhost:19000/project/wordcount/input
hdfs://localhost:19000/project/wordcount/output
右键，run as -&gt;  Java Application<br>
<img src="https://liangsj.xyz//post-images/1562314906961.png" alt=""></p>
<p>结束</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[hadoop 高可用配置]]></title>
        <id>https://liangsj.xyz//post/hadoop-gao-ke-yong-pei-zhi</id>
        <link href="https://liangsj.xyz//post/hadoop-gao-ke-yong-pei-zhi">
        </link>
        <updated>2019-07-05T08:16:15.000Z</updated>
        <content type="html"><![CDATA[<p>Hadoop1,只有一个NameNode，不可靠，容易单点故障。</p>
<p>Hddoop2后，使用高可用，可配置2个NameNode，一个活动，一个备用，
Hddoop2后，使用高可用，可配置2个NameNode，一个活动，一个备用，</p>
<p>当活动NameNode出现问题时，备用NameNode可手工或自动切换为活动状态</p>
<p>下面配置高可用:
<a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_hag_hdfs_ha_config.html">官方文档</a></p>
<h3 id="增加开启高可用">增加开启高可用</h3>
<p>1、在Cloudera Manager界面，进入HDFS服务，
2、操作-&gt; 启用High Availability
3、选择外一台配置与NameNode差不多的结点，安装NameNode服务，并安装JournalNode(至少安装到三台结点)</p>
<p>成功后为这样
<img src="https://liangsj.xyz//post-images/1562314595351.png" alt=""></p>
<h3 id="配置hive和huehbase等">配置HIVE和hue,hbase等</h3>
<p><a href="https://www.cloudera.com/documentation/enterprise/latest/topics/cdh_hag_hdfs_ha_cdh_components_config.html#concept_rj1_hsq_bp">官方文档</a></p>
<p>高可用后，需要重新配置HIVE和hue,hbase 等，这里只用到了hue和hive</p>
<ul>
<li>
<p>更新hive配置
1、进入hive服务页面
2、操作 -&gt; 停止
3、操作 -&gt; Update Hive Metastore NameNodes
4、操作 -&gt; 启动</p>
</li>
<li>
<p>配置Hue Web使用HttpFs
1、在HDFS服务页面增加HttpFS角色
2、进入Hue服务
3、配置 -&gt; Hue Web界面角色，选择HttpFs
如下
<img src="https://liangsj.xyz//post-images/1562314608435.png" alt=""></p>
</li>
</ul>
<p>4、重启Hue服务</p>
<p>完成</p>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[Yarn(MapReduce2.0)架构的资源请求流程]]></title>
        <id>https://liangsj.xyz//post/yarnmapreduce20jia-gou-de-zi-yuan-qing-qiu-liu-cheng</id>
        <link href="https://liangsj.xyz//post/yarnmapreduce20jia-gou-de-zi-yuan-qing-qiu-liu-cheng">
        </link>
        <updated>2019-07-05T07:57:26.000Z</updated>
        <content type="html"><![CDATA[<p>最近整理在做MapReduce开发，对Yarn的请求流程做些简单的整理，以防忘记。</p>
<p>MR1主要由JobTracker和TaskTacker组成，Yarn由ResourceManager、NodeManager、ApplicationMaster组成</p>
<p>以下是自己画的yarn流程图</p>
<p><img src="https://liangsj.xyz//post-images/1562313561494.png" alt=""></p>
<h3 id="一些词的解析">一些词的解析</h3>
<p><strong>ResourceManager：</strong> 集群老大，有最高权力，相当于老总，管理着整个集群的资源，谁要使用，都要先经过他。</p>
<ul>
<li>1、全局的资源管理器，负责整个系统的资源管理和分配。</li>
<li>2、它主要由两个组件构成：调度器（Scheduler）和应用程序管理器（Applications Manager，ASM）</li>
</ul>
<p><strong>NodeManager：</strong> 一台机器上的老大，当台机器上的管理者。相当于部分经理。</p>
<ul>
<li>1、每个节点上的资源和任务管理器。</li>
<li>2、定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；</li>
<li>3、接收并处理来自AM的Container启动/停止等各种请求</li>
</ul>
<p><strong>ApplicationMaster：</strong> 中间者，每个应用程序一个AM</p>
<ul>
<li>1、获取资源(用Container表示)</li>
<li>2、将得到的任务进一步分配给内部的任务(资源的二次分配)；</li>
<li>3、与NM通信以启动/停止任务；</li>
<li>4、监控所有任务运行状态，并在任务运行失败时重新为任务申请资源以重启任务</li>
</ul>
<p><strong>Container：</strong> 是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等</p>
<h3 id="流程">流程</h3>
<ul>
<li>
<p>1、用户向yarn提交程序，即Mapredeuce程序，ResourceManager(资源管理器)接收到客户端MR运行请求</p>
</li>
<li>
<p>2、ResourceManager分配一个Container(资源)用来启动ApplicationMaster(程序管理员)，并告知NodeManager(节点管理员)，要求它在这个Container下启动ApplicationMaster</p>
</li>
<li>
<p>3、ApplicationMaster启动后，向ResourceManager发起注册请求</p>
</li>
<li>
<p>4、ApplicationMaster向ResourceManager申请资源</p>
</li>
<li>
<p>5、取得资源后，根据资源，向相关的NodeManager通信，要求其启动程序</p>
</li>
<li>
<p>6、NodeManager（多个）启动MR</p>
</li>
<li>
<p>7、NodeManager不断汇报MR状态和进展给ApplicationMaster</p>
</li>
<li>
<p>8、当MR全部完成时，ApplicationMaster向ResourceManager汇报任务完成，并注销自己</p>
</li>
</ul>
]]></content>
    </entry>
    <entry>
        <title type="html"><![CDATA[git在windows下安装与使用]]></title>
        <id>https://liangsj.xyz//post/git-zai-windows-xia-an-zhuang-yu-shi-yong</id>
        <link href="https://liangsj.xyz//post/git-zai-windows-xia-an-zhuang-yu-shi-yong">
        </link>
        <updated>2019-07-05T07:22:51.000Z</updated>
        <content type="html"><![CDATA[<p>github桌面程序 使用过一段时间，太慢了，而且在家还安装不了，所以还是git命令好用，记录下安装和使用过程</p>
<h3 id="安装git客户端">安装Git客户端</h3>
<ul>
<li>
<p>到<a href="https://git-scm.com/downloads">git官网</a>下载git，并安装</p>
</li>
<li>
<p>GitHub上添加 SSH key</p>
<p>打开git bash，执行如下命令</p>
</li>
</ul>
<pre><code>$cd ~/.ssh    --没有则创建
$ssh-keygen -t rsa -b 4096 -C &quot;trgree@foxmail.com&quot;       --并连续按3个enter
</code></pre>
<p>复制~/.ssh目录下id_rsa.pub内容到github的ssh设置</p>
<p>位置： settings -&gt; SSH and GPG keys -&gt; new SSH key</p>
<p>测试连接是否成功</p>
<pre><code>$ssh -T git@github.com
</code></pre>
<p>输入yes，
如果出现以下提示证明添加成功</p>
<pre><code>Hi Trgree! You've successfully authenticated, but GitHub does not provide shell access.
</code></pre>
<ul>
<li>设置git</li>
</ul>
<pre><code>$git config --global user.name &quot;trgree&quot;                  # 请换成你自己的名字，除非你凑巧也叫wukong.sun 
$git config --global user.email &quot;trgree@foxmail.com&quot;         # 同上 
$git config --global push.default simple               # 要是你非要用低版本的Git（比如1.7.x），好吧，那就不设simple设current，否则你的Git不支持 
$git config --global core.autocrlf false               # 让Git不要管Windows/Unix换行符转换的事 
$git config --global gui.encoding utf-8                # 避免git gui中的中文乱码 
$git config --global core.quotepath off                # 避免git status显示的中文文件名乱码 
</code></pre>
<h3 id="克隆项目到本地">克隆项目到本地</h3>
<p>在github上获取项目git路径</p>
<pre><code>$git clone https://github.com/Trgree/trgree.github.io.git trgree.github.io
</code></pre>
<h3 id="查看文件状态有哪些文件是需要提交的">查看文件状态，有哪些文件是需要提交的</h3>
<pre><code>$cd trgree.github.io
$git status
</code></pre>
<p><img src="https://liangsj.xyz//post-images/1562311483071.png" alt=""></p>
<h3 id="添加要提交文件到缓存中">添加要提交文件到缓存中</h3>
<pre><code>$git add _posts/*
$git add images/posts/git-mark/pic1.png
$git add .   #当前目录下所有文件
$git commit -m 'add git-mark'
</code></pre>
<h3 id="提交修改后文件到github上">提交修改后文件到github上</h3>
<pre><code>$git push
</code></pre>
<p>输入用户密码即可。</p>
<p>但每次push都要输入用户密码，很麻烦，可以把push修改为ssh方式，则不用输入（前提是添加了ssh key到github，见前面）</p>
<p>通过以下命令查看</p>
<pre><code>git remote -v
</code></pre>
<p>返回</p>
<pre><code>origin  https://github.com/Trgree/trgree.github.io.git (fetch)
origin  https://github.com/Trgree/trgree.github.io.git (push)
</code></pre>
<p>修改为ssh</p>
<pre><code>git remote rm origin
git remote add origin git@github.com:Trgree/trgree.github.io.git
git push --set-upstream origin master
</code></pre>
<h3 id="更新远程文件到本地">更新远程文件到本地</h3>
<pre><code>$git pull
</code></pre>
]]></content>
    </entry>
</feed>